# ðŸ“Œ SGD with DataLoader in PyTorch

## ðŸš€ Overview
This repository implements **Stochastic Gradient Descent (SGD)** in PyTorch using `DataLoader` to efficiently iterate over training data. It includes a visualisation of the loss surface and parameter updates to better understand gradient descent optimisation.

## ðŸ”¥ Features
âœ” Implements **Linear Regression** using PyTorch  
âœ” Uses **Stochastic Gradient Descent (SGD)** for optimisation  
âœ” Utilizes `Dataset` and `DataLoader` for efficient data handling  
âœ” Compares **SGD with and without DataLoader**  
âœ” Visualises:
   - Data distribution  
   - Loss surface (3D & Contour plots)  
   - Model training progress  

## ðŸ“Š Visualisations
The repository includes plots to analyze:  
- The **loss surface**, showing how loss changes with different weights and biases  
- The **training process**, visualising how parameters update over time  


## ðŸ“š Learning Points
How SGD updates weights after every data sample
How using DataLoader improves efficiency
How visualising loss surfaces helps understand model convergence